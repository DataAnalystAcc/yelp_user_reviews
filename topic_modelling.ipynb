{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c776e603",
   "metadata": {},
   "source": [
    "BERTopic Topic Modelling â†’ creates topics on a subset of data and classifies review texts into the topics\n",
    "\n",
    " - created a designated distributed sample to preserve niche topics with stratification\n",
    "\n",
    " - a customized stop words list\n",
    "\n",
    " - minimum of 50 documents per cluster\n",
    "\n",
    " - filter to create more meaningful topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7b5f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exluding the training data\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, inspect, text\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from dotenv import dotenv_values\n",
    "from tqdm import tqdm # Import tqdm for progress bars\n",
    "\n",
    "# Import specific BERTopic sub-models for fine-tuning\n",
    "# from umap import UMAP # UMAP is implicitly used by BERTopic, no direct import needed unless customizing.\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from bertopic.representation import KeyBERTInspired, ZeroShotClassification, TextGeneration # Uncomment if you want to use advanced representation models\n",
    "\n",
    "# --- Configuration and Setup ---\n",
    "\n",
    "# Load environment variables from .env file\n",
    "config = dotenv_values()\n",
    "\n",
    "# PostgreSQL database connection details\n",
    "pg_user = config['POSTGRES_USER']\n",
    "pg_host = config['POSTGRES_HOST']\n",
    "pg_port = config['POSTGRES_PORT']\n",
    "pg_db = config['POSTGRES_DB']\n",
    "pg_schema = config.get(\"POSTGRES_SCHEMA\", \"public\") # Defaults to 'public' if not found\n",
    "pg_pass = config['POSTGRES_PASS']\n",
    "\n",
    "# Validate environment variables\n",
    "if not all([pg_user, pg_host, pg_port, pg_db, pg_pass]):\n",
    "    raise ValueError(\"Missing one or more required PostgreSQL environment variables (POSTGRES_USER, POSTGRES_HOST, POSTGRES_PORT, POSTGRES_DB, POSTGRES_PASS).\")\n",
    "if pg_schema == \"public\":\n",
    "    print(\"WARNING: POSTGRES_SCHEMA not found in .env, defaulting to 'public'.\")\n",
    "\n",
    "# Constants for file paths, table names, and batch processing\n",
    "CHUNK_SIZE = 10_000 # Number of rows to process in each batch for transformation\n",
    "INPUT_DB_TABLE = \"review_2019\" # Table containing source review data (all data to be analyzed)\n",
    "OUTPUT_DB_TABLE = \"bertopic_analysis_results_update_3\" # Table to store topic modeling results\n",
    "MODEL_DIR = \"bertopic_model_update_3\" # Directory to save the trained BERTopic model\n",
    "TOPIC_JSON_PATH = \"topic_keywords_update_3.json\" # Path to save topic summary information\n",
    "\n",
    "\n",
    "TRAINING_SAMPLE_SOURCE = \"DB_TABLE\"\n",
    "TRAINING_DB_TABLE_NAME = \"training_sample\" # <--- Specify your table name here (e.g., \"review_training_subset\")\n",
    "\n",
    "# --- Database Utility Functions ---\n",
    "\n",
    "def get_database_connection():\n",
    "    \"\"\"\n",
    "    Establishes and returns a SQLAlchemy database engine for PostgreSQL.\n",
    "    The connection string is constructed from environment variables.\n",
    "\n",
    "    Returns:\n",
    "        sqlalchemy.engine.base.Engine: A SQLAlchemy engine object.\n",
    "    \"\"\"\n",
    "    url = f\"postgresql+psycopg2://{pg_user}:{pg_pass}@{pg_host}:{pg_port}/{pg_db}\"\n",
    "    return create_engine(url, pool_pre_ping=True)\n",
    "\n",
    "def create_results_table(engine, schema_name, table_name):\n",
    "    \"\"\"\n",
    "    Creates a PostgreSQL table to store the BERTopic analysis results if it doesn't already exist.\n",
    "\n",
    "    Args:\n",
    "        engine (sqlalchemy.engine.base.Engine): The SQLAlchemy engine.\n",
    "        schema_name (str): The name of the database schema.\n",
    "        table_name (str): The name of the table to create.\n",
    "    \"\"\"\n",
    "    inspector = inspect(engine)\n",
    "    if inspector.has_table(table_name, schema=schema_name):\n",
    "        print(f\"Table '{schema_name}.{table_name}' already exists. Skipping creation.\")\n",
    "        return\n",
    "\n",
    "    # SQL DDL statement to create the table\n",
    "    create_table_sql = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {schema_name}.{table_name} (\n",
    "        business_id VARCHAR(255),\n",
    "        review_id VARCHAR(255) PRIMARY KEY,\n",
    "        text TEXT,\n",
    "        topic INTEGER,\n",
    "        probability NUMERIC(5, 4)\n",
    "    );\n",
    "    \"\"\"\n",
    "    with engine.connect() as connection:\n",
    "        connection.execute(text(create_table_sql))\n",
    "        connection.commit()\n",
    "    print(f\"Successfully created table '{schema_name}.{table_name}' for results.\")\n",
    "\n",
    "def load_reviews_from_db(engine, schema, table):\n",
    "    \"\"\"\n",
    "    Loads all reviews from a specified database table into a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        engine (sqlalchemy.engine.base.Engine): The SQLAlchemy engine.\n",
    "        schema (str): The name of the database schema.\n",
    "        table (str): The name of the table to load data from.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame containing the review data.\n",
    "    \"\"\"\n",
    "    query = f'SELECT review_id, text, business_id FROM \"{schema}\".\"{table}\"' # Select specific columns\n",
    "    return pd.read_sql_query(query, engine)\n",
    "\n",
    "def save_topic_info(model, path):\n",
    "    \"\"\"\n",
    "    Retrieves and saves the BERTopic model's topic information to a JSON file.\n",
    "\n",
    "    Args:\n",
    "        model (bertopic.BERTopic): The trained BERTopic model.\n",
    "        path (str): The file path to save the JSON output.\n",
    "    \"\"\"\n",
    "    topic_info = model.get_topic_info()\n",
    "    topic_info.to_json(path, orient=\"records\", indent=2)\n",
    "\n",
    "def write_batch_to_db(engine, df, table, schema):\n",
    "    \"\"\"\n",
    "    Writes a batch of DataFrame rows to a specified database table using psycopg2.extras.execute_values\n",
    "    for efficient bulk updates.\n",
    "\n",
    "    Args:\n",
    "        engine (sqlalchemy.engine.base.Engine): The SQLAlchemy engine.\n",
    "        df (pandas.DataFrame): The DataFrame batch to write. Must contain 'review_id', 'topic', 'probability'.\n",
    "        table (str): The name of the target database table.\n",
    "        schema (str): The name of the database schema.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return\n",
    "\n",
    "    # Prepare data for bulk update: list of tuples (topic, probability, review_id)\n",
    "    # The order must match the order in the SQL UPDATE statement.\n",
    "    # Ensure data types are consistent for PostgreSQL.\n",
    "    # business_id is also part of the insert.\n",
    "    data_to_update = [(row[\"business_id\"], row[\"review_id\"], row[\"text\"], int(row[\"topic\"]), float(row[\"probability\"]))\n",
    "                      for _, row in df.iterrows()]\n",
    "\n",
    "    with engine.begin() as conn:\n",
    "        # Get the underlying psycopg2 connection for execute_values\n",
    "        db_connection = conn.connection\n",
    "        cursor = db_connection.cursor()\n",
    "\n",
    "        # SQL for bulk upsert (INSERT or UPDATE if conflict on review_id)\n",
    "        # This prevents primary key errors if some data was already processed\n",
    "        # or if rerunning with overlapping chunks.\n",
    "        insert_update_sql = f\"\"\"\n",
    "            INSERT INTO \"{schema}\".\"{table}\" (business_id, review_id, text, topic, probability)\n",
    "            VALUES %s\n",
    "            ON CONFLICT (review_id) DO UPDATE SET\n",
    "                business_id = EXCLUDED.business_id,\n",
    "                text = EXCLUDED.text,\n",
    "                topic = EXCLUDED.topic,\n",
    "                probability = EXCLUDED.probability;\n",
    "        \"\"\"\n",
    "        # Execute the upsert statement using execute_values for performance\n",
    "        psycopg2.extras.execute_values(\n",
    "            cursor,\n",
    "            insert_update_sql,\n",
    "            data_to_update,\n",
    "            template=\"(%s, %s, %s, %s, %s)\", # Template for values (business_id, review_id, text, topic, probability)\n",
    "            page_size=CHUNK_SIZE # Controls internal batching for psycopg2\n",
    "        )\n",
    "        # No explicit commit needed here if using `engine.begin()` as it commits automatically on exit.\n",
    "        print(f\"Bulk inserted/updated {len(df)} rows with topic results into {table}.\")\n",
    "\n",
    "\n",
    "# --- Main Script Execution ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    engine = get_database_connection()\n",
    "    create_results_table(engine, pg_schema, OUTPUT_DB_TABLE)\n",
    "\n",
    "    print(\"Loading all review data from DB...\")\n",
    "    df_full = load_reviews_from_db(engine, pg_schema, INPUT_DB_TABLE)\n",
    "\n",
    "    # Drop exact duplicate text entries from the full dataset\n",
    "    # `keep='first'` keeps one instance of each duplicate, which is generally what you want\n",
    "    # to maintain data integrity while removing redundant text entries for modeling.\n",
    "    print(f\"Initial dataset size: {len(df_full)} rows.\")\n",
    "    df_full = df_full.drop_duplicates(subset='text', keep='first').copy()\n",
    "    print(f\"Dataset size after dropping exact text duplicates: {len(df_full)} rows.\")\n",
    "\n",
    "    # --- Select Training Data based on TRAINING_SAMPLE_SOURCE ---\n",
    "    df_sample = pd.DataFrame() # Initialize an empty DataFrame\n",
    "    if TRAINING_SAMPLE_SOURCE is None:\n",
    "        print(f\"Sampling {TRAINING_SAMPLE_SIZE} reviews randomly for training...\")\n",
    "        if len(df_full) < TRAINING_SAMPLE_SIZE:\n",
    "            print(f\"WARNING: Full dataset ({len(df_full)}) is smaller than requested sample size ({TRAINING_SAMPLE_SIZE}). Using full dataset for training.\")\n",
    "            df_sample = df_full.copy()\n",
    "        else:\n",
    "            df_sample = df_full.sample(n=TRAINING_SAMPLE_SIZE, random_state=42).copy()\n",
    "    elif TRAINING_SAMPLE_SOURCE == \"DB_TABLE\":\n",
    "        if 'TRAINING_DB_TABLE_NAME' not in locals() and 'TRAINING_DB_TABLE_NAME' not in globals():\n",
    "             raise ValueError(\"TRAINING_DB_TABLE_NAME must be defined in constants when TRAINING_SAMPLE_SOURCE is 'DB_TABLE'.\")\n",
    "        print(f\"Loading training data from database table '{pg_schema}.{TRAINING_DB_TABLE_NAME}'...\")\n",
    "        try:\n",
    "            df_sample = load_reviews_from_db(engine, pg_schema, TRAINING_DB_TABLE_NAME)\n",
    "            if df_sample.empty:\n",
    "                raise ValueError(f\"No data found in training table '{pg_schema}.{TRAINING_DB_TABLE_NAME}'.\")\n",
    "            if 'review_id' not in df_sample.columns or 'text' not in df_sample.columns:\n",
    "                raise ValueError(f\"Training table '{pg_schema}.{TRAINING_DB_TABLE_NAME}' must contain 'review_id' and 'text' columns.\")\n",
    "            # Ensure the sample reviews are also unique by text if they came from an external table\n",
    "            original_sample_size = len(df_sample)\n",
    "            df_sample = df_sample.drop_duplicates(subset='text', keep='first').copy()\n",
    "            if len(df_sample) < original_sample_size:\n",
    "                print(f\"INFO: Removed {original_sample_size - len(df_sample)} duplicate texts from training sample loaded from DB table.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error loading training sample from database table '{TRAINING_DB_TABLE_NAME}': {e}\")\n",
    "    elif isinstance(TRAINING_SAMPLE_SOURCE, list):\n",
    "        print(\"Using provided list of review_ids for training data...\")\n",
    "        df_sample = df_full[df_full[\"review_id\"].isin(TRAINING_SAMPLE_SOURCE)].copy()\n",
    "        if len(df_sample) != len(TRAINING_SAMPLE_SOURCE):\n",
    "            print(f\"WARNING: Not all provided review_ids ({len(TRAINING_SAMPLE_SOURCE)}) were found in the full dataset. Found {len(df_sample)}.\")\n",
    "    elif isinstance(TRAINING_SAMPLE_SOURCE, str):\n",
    "        print(f\"Loading training data from '{TRAINING_SAMPLE_SOURCE}' (CSV file)...\")\n",
    "        try:\n",
    "            df_source_ids = pd.read_csv(TRAINING_SAMPLE_SOURCE)\n",
    "            if \"review_id\" not in df_source_ids.columns:\n",
    "                raise ValueError(f\"CSV sample file '{TRAINING_SAMPLE_SOURCE}' must contain a 'review_id' column.\")\n",
    "\n",
    "            # Filter df_full to get the actual texts for the specified review_ids\n",
    "            df_sample = df_full[df_full[\"review_id\"].isin(df_source_ids[\"review_id\"])].copy()\n",
    "\n",
    "            if len(df_sample) != len(df_source_ids):\n",
    "                print(f\"WARNING: Not all review_ids from '{TRAINING_SAMPLE_SOURCE}' ({len(df_source_ids)}) were found in the (deduplicated) database data. Using {len(df_sample)} IDs found.\")\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"Training sample file not found at '{TRAINING_SAMPLE_SOURCE}'\")\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error loading training sample from CSV: {e}\")\n",
    "    else:\n",
    "        raise ValueError(\"Invalid type for TRAINING_SAMPLE_SOURCE. Must be None, 'DB_TABLE', a list of review_ids, or a file path string.\")\n",
    "\n",
    "    if df_sample.empty:\n",
    "        raise ValueError(\"No training data could be loaded. Please check your TRAINING_SAMPLE_SOURCE configuration and data.\")\n",
    "\n",
    "    sampled_texts = df_sample[\"text\"].tolist()\n",
    "    print(f\"Training BERTopic model on {len(sampled_texts)} reviews.\")\n",
    "\n",
    "    print(\"Training BERTopic model with customized settings...\")\n",
    "    # Load embedding model\n",
    "    embedding_model = SentenceTransformer(\"intfloat/e5-large-v2\")\n",
    "\n",
    "    # --- Customize BERTopic Parameters Here ---\n",
    "    # These settings are crucial for improving topic quality, especially for large datasets.\n",
    "\n",
    "    # 1. Custom CountVectorizer for stop words and min_df\n",
    "    # - `stop_words`: Using a custom list of stop words to remove very common words that\n",
    "    #   might not be useful for distinguishing topics.\n",
    "    # - `min_df=10`: Ignores terms that appear in fewer than 10 documents (helps reduce noise).\n",
    "    # - `ngram_range=(1, 2)`: Includes single words (unigrams) and two-word phrases (bigrams)\n",
    "    #   as potential topic keywords, which can capture more nuanced topic representations.\n",
    "    custom_stop_words = [\n",
    "        \"a\", \"an\", \"the\", \"and\", \"or\", \"but\",\n",
    "        \"if\", \"while\", \"is\", \"am\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\",\n",
    "        \"do\", \"does\", \"did\", \"doing\",\n",
    "        \"have\", \"has\", \"had\", \"having\",\n",
    "        \"i\", \"me\", \"my\", \"myself\",\n",
    "        \"we\", \"our\", \"ours\", \"ourselves\",\n",
    "        \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\",\n",
    "        \"he\", \"him\", \"his\", \"himself\",\n",
    "        \"she\", \"her\", \"hers\", \"herself\",\n",
    "        \"it\", \"its\", \"itself\",\n",
    "        \"they\", \"them\", \"their\", \"theirs\", \"themselves\",\n",
    "        \"this\", \"that\", \"these\", \"those\",\n",
    "        \"to\", \"in\", \"of\",\"for\",\"on\",\"at\",\"with\",\n",
    "        \"as\", \"such\", \"too\",\n",
    "        \"can\", \"will\", \"would\", \"should\", \"could\",\n",
    "        \"just\", \"only\", \"also\", \"so\", \"than\", \"then\", \"there\", \"here\",\n",
    "        \"what\", \"which\", \"who\", \"whom\", \"whose\",\n",
    "        \"when\", \"where\", \"why\", \"how\"\n",
    "    ]\n",
    "\n",
    "    vectorizer_model = CountVectorizer(stop_words=custom_stop_words, min_df=10, ngram_range=(1, 2))\n",
    "\n",
    "    # 2. Custom HDBSCAN for clustering\n",
    "    # - `min_cluster_size=50`: This is your `min_topic_size`. Increasing it (from default 10)\n",
    "    #   helps reduce outliers (-1 topic) and yields more robust and larger topics.\n",
    "    #   For a million rows, 50 is a reasonable starting point, but you might even go higher (e.g., 100-200)\n",
    "    #   depending on the desired granularity of topics.\n",
    "    # - `prediction_data=True`: Essential for the `.transform()` method to assign new documents to topics,\n",
    "    #   especially for documents not in the training set.\n",
    "    hdbscan_model = HDBSCAN(min_cluster_size=50, prediction_data=True)\n",
    "\n",
    "    # 3. Initialize BERTopic with all customized models and settings\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=embedding_model,\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        hdbscan_model=hdbscan_model,\n",
    "        nr_topics=\"auto\", # Automatically merge topics to a more optimal count.\n",
    "                           # Alternatively, you can specify an integer, e.g., nr_topics=100.\n",
    "        language=\"english\", # Explicitly set language for the model.\n",
    "        verbose=True        # Show progress updates during training.\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    # Fit the model to your selected training data\n",
    "    topics, probabilities = topic_model.fit_transform(sampled_texts)\n",
    "    print(f\"BERTopic model training complete. Found {len(topic_model.get_topics())} topics.\")\n",
    "\n",
    "\n",
    "    # Save the trained model and topic information\n",
    "    print(f\"Saving BERTopic model to {MODEL_DIR}...\")\n",
    "    topic_model.save(MODEL_DIR) # Saves the model configuration and trained components\n",
    "    print(f\"Saving topic summary to {TOPIC_JSON_PATH}...\")\n",
    "    save_topic_info(topic_model, TOPIC_JSON_PATH)\n",
    "\n",
    "    # --- Process Remaining Data in Chunks ---\n",
    "\n",
    "    # Filter out training data from the full dataset so it's not processed again\n",
    "    print(\"Filtering out training data from the full dataset for batch processing...\")\n",
    "    # This ensures that only reviews *not* used for training are processed in the next step.\n",
    "    df_remaining = df_full[~df_full[\"review_id\"].isin(df_sample[\"review_id\"])].copy()\n",
    "    print(f\"Remaining reviews to process with topic model: {len(df_remaining)} rows.\")\n",
    "\n",
    "    # Loop through the remaining data in defined CHUNK_SIZE batches\n",
    "    # Use tqdm for a main progress bar for processing the remaining data\n",
    "    with tqdm(total=len(df_remaining), desc=\"Processing Remaining Reviews\", unit=\"reviews\") as pbar_remaining:\n",
    "        for start in range(0, len(df_remaining), CHUNK_SIZE):\n",
    "            end = start + CHUNK_SIZE\n",
    "            batch = df_remaining.iloc[start:end].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
    "\n",
    "            texts_batch = batch[\"text\"].tolist()\n",
    "            # Transform the batch using the trained model to get topic assignments\n",
    "            # Note: topic_model.transform() is used for new data, not .fit_transform()\n",
    "            topics_batch, probs_batch = topic_model.transform(texts_batch)\n",
    "\n",
    "            # Add topic and probability results to the batch DataFrame\n",
    "            batch[\"topic\"] = topics_batch\n",
    "            batch[\"probability\"] = probs_batch\n",
    "\n",
    "            # Write the processed batch results to the database\n",
    "            # Ensure only relevant columns are passed to write_batch_to_db\n",
    "            write_batch_to_db(engine, batch[[\"business_id\", \"review_id\", \"text\", \"topic\", \"probability\"]],\n",
    "                              OUTPUT_DB_TABLE, pg_schema)\n",
    "\n",
    "            pbar_remaining.update(len(batch)) # Update progress bar for the processed batch\n",
    "            print(f\"Processed and updated {len(batch)} rows.\")\n",
    "\n",
    "    print(\"Finished topic modeling and database update for all reviews.\")\n",
    "\n",
    "    # Close the database connection\n",
    "    engine.dispose()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
