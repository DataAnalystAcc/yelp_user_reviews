{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c36f55c9",
   "metadata": {},
   "source": [
    "Topic Modelling for review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff3a335",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load 1000 Yelp review entries\n",
    "records = []\n",
    "with open(\"./data/yelp_academic_dataset_review.json\", \"r\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= 1000:  # Change this to load more/less\n",
    "            break\n",
    "        try:\n",
    "            record = json.loads(line)\n",
    "            records.append(record)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Skipping bad line {i}\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_reviews = pd.DataFrame(records)\n",
    "\n",
    "# Extract the review text column\n",
    "documents = df_reviews['text'].dropna().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e5b30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691b5032",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install -c conda-forge wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34705ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run in terminal\n",
    "# conda install -c conda-forge spacy\n",
    "# python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bb6fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Load spaCy model (install via: pip install spacy && python -m spacy download en_core_web_sm)\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "# --- Load example set of 1000 Yelp reviews ---\n",
    "records = []\n",
    "with open(\"./data/yelp_academic_dataset_review.json\", \"r\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= 1000:\n",
    "            break\n",
    "        try:\n",
    "            record = json.loads(line)\n",
    "            records.append(record)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Skipping bad line {i}\")\n",
    "\n",
    "df_reviews = pd.DataFrame(records)\n",
    "documents = df_reviews['text'].dropna().tolist()\n",
    "\n",
    "# --- Preprocessing with spaCy ---\n",
    "def spacy_preprocess(text):\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)  # Remove non-letters\n",
    "    doc = nlp(text.lower())\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and token.is_alpha and len(token) > 2]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "preprocessed_docs = [spacy_preprocess(doc) for doc in documents]\n",
    "\n",
    "# --- Vectorize Text ---\n",
    "vectorizer = CountVectorizer(max_df=0.9, min_df=10, stop_words='english')  # min_df=10 to ignore rare terms\n",
    "doc_term_matrix = vectorizer.fit_transform(preprocessed_docs)\n",
    "\n",
    "# --- Fit LDA Model ---\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "lda.fit(doc_term_matrix)\n",
    "\n",
    "# --- Show Topics ---\n",
    "def print_topics(model, vectorizer, top_n=10):\n",
    "    words = vectorizer.get_feature_names_out()\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(f\"\\nTopic #{idx + 1}:\")\n",
    "        print([words[i] for i in topic.argsort()[-top_n:]])\n",
    "\n",
    "print_topics(lda, vectorizer)\n",
    "\n",
    "# --- Optional: Word Clouds ---\n",
    "for idx, topic in enumerate(lda.components_):\n",
    "    plt.figure()\n",
    "    plt.imshow(WordCloud(background_color='white')\n",
    "               .fit_words({vectorizer.get_feature_names_out()[i]: topic[i] for i in topic.argsort()[-15:]}))\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Topic #{idx + 1}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0946ff",
   "metadata": {},
   "source": [
    "Sentiment Analysis on review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d87d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Download VADER lexicon (only once)\n",
    "# import nltk\n",
    "# nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36de1a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load reviews (already provided in your code)\n",
    "records = []\n",
    "with open(\"./data/yelp_academic_dataset_review.json\", \"r\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= 1000:\n",
    "            break\n",
    "        try:\n",
    "            record = json.loads(line)\n",
    "            records.append(record)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Skipping bad line {i}\")\n",
    "\n",
    "df_reviews = pd.DataFrame(records)\n",
    "documents = df_reviews['text'].dropna().tolist()\n",
    "\n",
    "# Initialize VADER sentiment analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Compute sentiment scores\n",
    "sentiment_scores = [sia.polarity_scores(text) for text in documents]\n",
    "\n",
    "# Convert scores to DataFrame and combine with original reviews\n",
    "df_sentiments = pd.DataFrame(sentiment_scores)\n",
    "df_reviews_sentiment = pd.concat([df_reviews.reset_index(drop=True), df_sentiments], axis=1)\n",
    "\n",
    "# Add a label for sentiment (positive, neutral, negative)\n",
    "def label_sentiment(score):\n",
    "    if score >= 0.05:\n",
    "        return \"positive\"\n",
    "    elif score <= -0.05:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "df_reviews_sentiment[\"sentiment\"] = df_reviews_sentiment[\"compound\"].apply(label_sentiment)\n",
    "\n",
    "# Preview results\n",
    "print(df_reviews_sentiment[[\"text\", \"compound\", \"sentiment\"]].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e6d6d6",
   "metadata": {},
   "source": [
    "Visualizations for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b96db0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# --- Visualization 1: Countplot of sentiment labels ---\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(\n",
    "    data=df_reviews_sentiment,\n",
    "    x=\"sentiment\",\n",
    "    hue=\"sentiment\",          # Assign x to hue\n",
    "    order=[\"positive\", \"neutral\", \"negative\"],\n",
    "    palette=\"Set2\",\n",
    "    legend=False              # Hide redundant legend\n",
    ")\n",
    "plt.title(\"Sentiment Distribution\")\n",
    "plt.xlabel(\"Sentiment\")\n",
    "plt.ylabel(\"Number of Reviews\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Visualization 2: Histogram of compound scores ---\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.histplot(data=df_reviews_sentiment, x=\"compound\", kde=True, bins=30, color=\"skyblue\")\n",
    "plt.title(\"Distribution of Compound Sentiment Scores\")\n",
    "plt.xlabel(\"Compound Score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360ce067",
   "metadata": {},
   "source": [
    "Tip instead of review Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b0bbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Load spaCy model (install via: pip install spacy && python -m spacy download en_core_web_sm)\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "# --- Load example set of 1000 Yelp reviews ---\n",
    "records = []\n",
    "with open(\"./data/yelp_academic_dataset_tip.json\", \"r\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= 1000:\n",
    "            break\n",
    "        try:\n",
    "            record = json.loads(line)\n",
    "            records.append(record)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Skipping bad line {i}\")\n",
    "\n",
    "df_reviews = pd.DataFrame(records)\n",
    "documents = df_reviews['text'].dropna().tolist()\n",
    "\n",
    "# --- Preprocessing with spaCy ---\n",
    "def spacy_preprocess(text):\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)  # Remove non-letters\n",
    "    doc = nlp(text.lower())\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and token.is_alpha and len(token) > 2]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "preprocessed_docs = [spacy_preprocess(doc) for doc in documents]\n",
    "\n",
    "# --- Vectorize Text ---\n",
    "vectorizer = CountVectorizer(max_df=0.9, min_df=10, stop_words='english')  # min_df=10 to ignore rare terms\n",
    "doc_term_matrix = vectorizer.fit_transform(preprocessed_docs)\n",
    "\n",
    "# --- Fit LDA Model ---\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "lda.fit(doc_term_matrix)\n",
    "\n",
    "# --- Show Topics ---\n",
    "def print_topics(model, vectorizer, top_n=10):\n",
    "    words = vectorizer.get_feature_names_out()\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(f\"\\nTopic #{idx + 1}:\")\n",
    "        print([words[i] for i in topic.argsort()[-top_n:]])\n",
    "\n",
    "print_topics(lda, vectorizer)\n",
    "\n",
    "# --- Optional: Word Clouds ---\n",
    "for idx, topic in enumerate(lda.components_):\n",
    "    plt.figure()\n",
    "    plt.imshow(WordCloud(background_color='white')\n",
    "               .fit_words({vectorizer.get_feature_names_out()[i]: topic[i] for i in topic.argsort()[-15:]}))\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Topic #{idx + 1}\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
