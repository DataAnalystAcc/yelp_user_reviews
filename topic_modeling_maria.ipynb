{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acec3475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created table 'capstone_yelp.bertopic_analysis_results_update_4' for results.\n",
      "Loading all review data from DB...\n",
      "Initial dataset size: 907284 rows.\n",
      "Dataset size after dropping exact text duplicates: 905331 rows.\n",
      "Loading training data from database table 'capstone_yelp.training_sample'...\n",
      "INFO: Removed 131 duplicate texts from training sample loaded from DB table.\n",
      "Training BERTopic model on 199869 reviews.\n",
      "MPS (Metal Performance Shaders) is available! Using Apple Silicon GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs.hf.co/repos/de/b9/deb90e29ca6ddad4c7f23805a5a02770ba9d3cc3054242541d4fc18a10fd5886/d741c1a688a6169af0ecb5a047c44645cd992c31e1bf431269f98bba9ae2911a?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model.safetensors%3B+filename%3D%22model.safetensors%22%3B&Expires=1750092614&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MDA5MjYxNH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy9kZS9iOS9kZWI5MGUyOWNhNmRkYWQ0YzdmMjM4MDVhNWEwMjc3MGJhOWQzY2MzMDU0MjQyNTQxZDRmYzE4YTEwZmQ1ODg2L2Q3NDFjMWE2ODhhNjE2OWFmMGVjYjVhMDQ3YzQ0NjQ1Y2Q5OTJjMzFlMWJmNDMxMjY5Zjk4YmJhOWFlMjkxMWE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=NrFVjI56rnvjN-Lt0kageqk8jCIPrFInLHo0s1qjwOb1vrV1wX%7EAkjfVQHvq5j3OknKI5mfPYylvn3twKMv3WE08J3Wx3d6h0quTrBblT4g3fSUZwolpLucgB7C0koMn7d7mE3aSx9WtyQzmcJV%7EhvBDhEHIlRKn-lwXxzgo6cHS41hRx8y5vEUJD-5hYMhbEHg71SlFpB8txjSpxB%7EwFO47afb%7EExZh1mJ0FKKNisbPOlTGQAfosa1DrKed7xyxTPgt1pFNSi-1czmqnqLa0RCZN8O%7EvaxqrCP9vKHL8yuZmkaOxjp4gZHhhfdwW2z0HTYXg5zlPhyl607ZjBt8DQ__&Key-Pair-Id=K3RPWS32NSSJCE: HTTPSConnectionPool(host='cdn-lfs.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BERTopic model with customized settings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-16 18:33:48,706 - BERTopic - Embedding - Transforming documents to embeddings.\n",
      "Batches:  15%|█▍        | 931/6246 [15:26:37<88:10:00, 59.72s/it]  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, inspect, text\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from dotenv import dotenv_values\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import psycopg2.extras # Import psycopg2.extras for execute_values\n",
    "import torch # Import torch to check for MPS/CUDA availability\n",
    "\n",
    "# Import specific BERTopic sub-models for fine-tuning\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# --- Configuration and Setup ---\n",
    "\n",
    "# Load environment variables from .env file\n",
    "config = dotenv_values()\n",
    "\n",
    "# PostgreSQL database connection details\n",
    "pg_user = config['POSTGRES_USER']\n",
    "pg_host = config['POSTGRES_HOST']\n",
    "pg_port = config['POSTGRES_PORT']\n",
    "pg_db = config['POSTGRES_DB']\n",
    "pg_schema = config.get(\"POSTGRES_SCHEMA\", \"public\")\n",
    "pg_pass = config['POSTGRES_PASS']\n",
    "\n",
    "# Validate environment variables\n",
    "if not all([pg_user, pg_host, pg_port, pg_db, pg_pass]):\n",
    "    raise ValueError(\"Missing one or more required PostgreSQL environment variables (POSTGRES_USER, POSTGRES_HOST, POSTGRES_PORT, POSTGRES_DB, POSTGRES_PASS).\")\n",
    "if pg_schema == \"public\":\n",
    "    print(\"WARNING: POSTGRES_SCHEMA not found in .env, defaulting to 'public'.\")\n",
    "\n",
    "# Constants for file paths, table names, and batch processing\n",
    "CHUNK_SIZE = 10_000\n",
    "INPUT_DB_TABLE = \"review_2019\"\n",
    "OUTPUT_DB_TABLE = \"bertopic_analysis_results_update_4\"\n",
    "MODEL_DIR = \"bertopic_model_update_4\"\n",
    "TOPIC_JSON_PATH = \"topic_keywords_update_4.json\"\n",
    "\n",
    "# --- MODIFIED SAMPLE CONFIGURATION ---\n",
    "# Configure how your training sample is selected. Choose one of the following:\n",
    "\n",
    "# Option 1: Random Sampling\n",
    "# Set to None to use random sampling from the full dataset.\n",
    "# The `TRAINING_SAMPLE_SIZE` constant below will be used.\n",
    "# TRAINING_SAMPLE_SOURCE = None\n",
    "\n",
    "# Option 2: Use a specific list of review_ids\n",
    "# Uncomment the line below and replace with your list of review_ids.\n",
    "# Ensure these review_ids exist in your INPUT_DB_TABLE.\n",
    "# TRAINING_SAMPLE_SOURCE = [\"review_abc_123\", \"review_def_456\", \"review_ghi_789\"]\n",
    "\n",
    "# Option 3: Load sample from a CSV file\n",
    "# Uncomment the line below and replace with the path to your CSV file.\n",
    "# The CSV file MUST contain a column named \"review_id\".\n",
    "# It's recommended that it also contains the \"text\" column for clarity,\n",
    "# though the script will fetch the text from the database based on review_id.\n",
    "# TRAINING_SAMPLE_SOURCE = \"path/to/your_modified_sample.csv\"\n",
    "\n",
    "# Option 4: Load sample from a specific database table\n",
    "# Uncomment the line below. This table MUST contain 'review_id' and 'text' columns.\n",
    "TRAINING_SAMPLE_SOURCE = \"DB_TABLE\"\n",
    "TRAINING_DB_TABLE_NAME = \"training_sample\" # <--- Specify your table name here (e.g., \"review_training_subset\")\n",
    "\n",
    "\n",
    "# If using random sampling (TRAINING_SAMPLE_SOURCE = None), define the size here:\n",
    "#TRAINING_SAMPLE_SIZE = 200000\n",
    "\n",
    "# --- Database Utility Functions ---\n",
    "\n",
    "def get_database_connection():\n",
    "    \"\"\"\n",
    "    Establishes and returns a SQLAlchemy database engine for PostgreSQL.\n",
    "    The connection string is constructed from environment variables.\n",
    "\n",
    "    Returns:\n",
    "        sqlalchemy.engine.base.Engine: A SQLAlchemy engine object.\n",
    "    \"\"\"\n",
    "    url = f\"postgresql+psycopg2://{pg_user}:{pg_pass}@{pg_host}:{pg_port}/{pg_db}\"\n",
    "    return create_engine(url, pool_pre_ping=True)\n",
    "\n",
    "def create_results_table(engine, schema_name, table_name):\n",
    "    \"\"\"\n",
    "    Creates a PostgreSQL table to store the BERTopic analysis results if it doesn't already exist.\n",
    "\n",
    "    Args:\n",
    "        engine (sqlalchemy.engine.base.Engine): The SQLAlchemy engine.\n",
    "        schema_name (str): The name of the database schema.\n",
    "        table_name (str): The name of the table to create.\n",
    "    \"\"\"\n",
    "    inspector = inspect(engine)\n",
    "    if inspector.has_table(table_name, schema=schema_name):\n",
    "        print(f\"Table '{schema_name}.{table_name}' already exists. Skipping creation.\")\n",
    "        return\n",
    "\n",
    "    # SQL DDL statement to create the table\n",
    "    create_table_sql = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {schema_name}.{table_name} (\n",
    "        business_id VARCHAR(255),\n",
    "        review_id VARCHAR(255) PRIMARY KEY,\n",
    "        text TEXT,\n",
    "        topic INTEGER,\n",
    "        probability NUMERIC(5, 4)\n",
    "    );\n",
    "    \"\"\"\n",
    "    with engine.connect() as connection:\n",
    "        connection.execute(text(create_table_sql))\n",
    "        connection.commit()\n",
    "    print(f\"Successfully created table '{schema_name}.{table_name}' for results.\")\n",
    "\n",
    "def load_reviews_from_db(engine, schema, table):\n",
    "    \"\"\"\n",
    "    Loads all reviews from a specified database table into a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        engine (sqlalchemy.engine.base.Engine): The SQLAlchemy engine.\n",
    "        schema (str): The name of the database schema.\n",
    "        table (str): The name of the table to load data from.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame containing the review data.\n",
    "    \"\"\"\n",
    "    query = f'SELECT review_id, text, business_id FROM \"{schema}\".\"{table}\"'\n",
    "    return pd.read_sql_query(query, engine)\n",
    "\n",
    "def save_topic_info(model, path):\n",
    "    \"\"\"\n",
    "    Retrieves and saves the BERTopic model's topic information to a JSON file.\n",
    "\n",
    "    Args:\n",
    "        model (bertopic.BERTopic): The trained BERTopic model.\n",
    "        path (str): The file path to save the JSON output.\n",
    "    \"\"\"\n",
    "    topic_info = model.get_topic_info()\n",
    "    topic_info.to_json(path, orient=\"records\", indent=2)\n",
    "\n",
    "def write_batch_to_db(engine, df, table, schema):\n",
    "    \"\"\"\n",
    "    Writes a batch of DataFrame rows to a specified database table using psycopg2.extras.execute_values\n",
    "    for efficient bulk updates/inserts (upsert).\n",
    "\n",
    "    Args:\n",
    "        engine (sqlalchemy.engine.base.Engine): The SQLAlchemy engine.\n",
    "        df (pandas.DataFrame): The DataFrame batch to write. Must contain 'review_id', 'topic', 'probability',\n",
    "                               'business_id', and 'text'.\n",
    "        table (str): The name of the target database table.\n",
    "        schema (str): The name of the database schema.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return\n",
    "\n",
    "    # Prepare data for bulk upsert: list of tuples (business_id, review_id, text, topic, probability)\n",
    "    data_to_upsert = [(row[\"business_id\"], row[\"review_id\"], row[\"text\"], int(row[\"topic\"]), float(row[\"probability\"]))\n",
    "                      for _, row in df.iterrows()]\n",
    "\n",
    "    with engine.begin() as conn:\n",
    "        # Get the underlying psycopg2 connection for execute_values\n",
    "        db_connection = conn.connection\n",
    "        cursor = db_connection.cursor()\n",
    "\n",
    "        # SQL for bulk upsert (INSERT or UPDATE if conflict on review_id)\n",
    "        upsert_sql = f\"\"\"\n",
    "            INSERT INTO \"{schema}\".\"{table}\" (business_id, review_id, text, topic, probability)\n",
    "            VALUES %s\n",
    "            ON CONFLICT (review_id) DO UPDATE SET\n",
    "                business_id = EXCLUDED.business_id,\n",
    "                text = EXCLUDED.text,\n",
    "                topic = EXCLUDED.topic,\n",
    "                probability = EXCLUDED.probability;\n",
    "        \"\"\"\n",
    "        psycopg2.extras.execute_values(\n",
    "            cursor,\n",
    "            upsert_sql,\n",
    "            data_to_upsert,\n",
    "            template=\"(%s, %s, %s, %s, %s)\",\n",
    "            page_size=CHUNK_SIZE\n",
    "        )\n",
    "        print(f\"Bulk upserted {len(df)} rows with topic results into {table}.\")\n",
    "\n",
    "\n",
    "# --- Main Script Execution ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    engine = get_database_connection()\n",
    "    create_results_table(engine, pg_schema, OUTPUT_DB_TABLE)\n",
    "\n",
    "    print(\"Loading all review data from DB...\")\n",
    "    df_full = load_reviews_from_db(engine, pg_schema, INPUT_DB_TABLE)\n",
    "\n",
    "    print(f\"Initial dataset size: {len(df_full)} rows.\")\n",
    "    df_full = df_full.drop_duplicates(subset='text', keep='first').copy()\n",
    "    print(f\"Dataset size after dropping exact text duplicates: {len(df_full)} rows.\")\n",
    "\n",
    "    # --- Select Training Data based on TRAINING_SAMPLE_SOURCE ---\n",
    "    df_sample = pd.DataFrame()\n",
    "    if TRAINING_SAMPLE_SOURCE is None:\n",
    "        print(f\"Sampling {TRAINING_SAMPLE_SIZE} reviews randomly for training...\")\n",
    "        if len(df_full) < TRAINING_SAMPLE_SIZE:\n",
    "            print(f\"WARNING: Full dataset ({len(df_full)}) is smaller than requested sample size ({TRAINING_SAMPLE_SIZE}). Using full dataset for training.\")\n",
    "            df_sample = df_full.copy()\n",
    "        else:\n",
    "            df_sample = df_full.sample(n=TRAINING_SAMPLE_SIZE, random_state=42).copy()\n",
    "    elif TRAINING_SAMPLE_SOURCE == \"DB_TABLE\":\n",
    "        if 'TRAINING_DB_TABLE_NAME' not in locals() and 'TRAINING_DB_TABLE_NAME' not in globals():\n",
    "             raise ValueError(\"TRAINING_DB_TABLE_NAME must be defined in constants when TRAINING_SAMPLE_SOURCE is 'DB_TABLE'.\")\n",
    "        print(f\"Loading training data from database table '{pg_schema}.{TRAINING_DB_TABLE_NAME}'...\")\n",
    "        try:\n",
    "            df_sample = load_reviews_from_db(engine, pg_schema, TRAINING_DB_TABLE_NAME)\n",
    "            if df_sample.empty:\n",
    "                raise ValueError(f\"No data found in training table '{pg_schema}.{TRAINING_DB_TABLE_NAME}'.\")\n",
    "            if 'review_id' not in df_sample.columns or 'text' not in df_sample.columns:\n",
    "                raise ValueError(f\"Training table '{pg_schema}.{TRAINING_DB_TABLE_NAME}' must contain 'review_id' and 'text' columns.\")\n",
    "            original_sample_size = len(df_sample)\n",
    "            df_sample = df_sample.drop_duplicates(subset='text', keep='first').copy()\n",
    "            if len(df_sample) < original_sample_size:\n",
    "                print(f\"INFO: Removed {original_sample_size - len(df_sample)} duplicate texts from training sample loaded from DB table.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error loading training sample from database table '{TRAINING_DB_TABLE_NAME}': {e}\")\n",
    "    elif isinstance(TRAINING_SAMPLE_SOURCE, list):\n",
    "        print(\"Using provided list of review_ids for training data...\")\n",
    "        df_sample = df_full[df_full[\"review_id\"].isin(TRAINING_SAMPLE_SOURCE)].copy()\n",
    "        if len(df_sample) != len(TRAINING_SAMPLE_SOURCE):\n",
    "            print(f\"WARNING: Not all provided review_ids ({len(TRAINING_SAMPLE_SOURCE)}) were found in the full dataset. Found {len(df_sample)}.\")\n",
    "    elif isinstance(TRAINING_SAMPLE_SOURCE, str):\n",
    "        print(f\"Loading training data from '{TRAINING_SAMPLE_SOURCE}' (CSV file)...\")\n",
    "        try:\n",
    "            df_source_ids = pd.read_csv(TRAINING_SAMPLE_SOURCE)\n",
    "            if \"review_id\" not in df_source_ids.columns:\n",
    "                raise ValueError(f\"CSV sample file '{TRAINING_SAMPLE_SOURCE}' must contain a 'review_id' column.\")\n",
    "\n",
    "            df_sample = df_full[df_full[\"review_id\"].isin(df_source_ids[\"review_id\"])].copy()\n",
    "\n",
    "            if len(df_sample) != len(df_source_ids):\n",
    "                print(f\"WARNING: Not all review_ids from '{TRAINING_SAMPLE_SOURCE}' ({len(df_source_ids)}) were found in the (deduplicated) database data. Using {len(df_sample)} IDs found.\")\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"Training sample file not found at '{TRAINING_SAMPLE_SOURCE}'\")\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error loading training sample from CSV: {e}\")\n",
    "    else:\n",
    "        raise ValueError(\"Invalid type for TRAINING_SAMPLE_SOURCE. Must be None, 'DB_TABLE', a list of review_ids, or a file path string.\")\n",
    "\n",
    "    if df_sample.empty:\n",
    "        raise ValueError(\"No training data could be loaded. Please check your TRAINING_SAMPLE_SOURCE configuration and data.\")\n",
    "\n",
    "    sampled_texts = df_sample[\"text\"].tolist()\n",
    "    print(f\"Training BERTopic model on {len(sampled_texts)} reviews.\")\n",
    "\n",
    "    # --- Determine the optimal device for SentenceTransformer ---\n",
    "    embedding_device = 'cpu'\n",
    "    if torch.cuda.is_available():\n",
    "        embedding_device = 'cuda'\n",
    "        print(f\"CUDA is available! Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    elif torch.backends.mps.is_available(): # Check for Apple Silicon MPS\n",
    "        embedding_device = 'mps'\n",
    "        print(\"MPS (Metal Performance Shaders) is available! Using Apple Silicon GPU.\")\n",
    "    else:\n",
    "        print(\"No GPU (CUDA or MPS) detected. Falling back to CPU for embeddings.\")\n",
    "\n",
    "    # Load embedding model, explicitly telling it which device to use\n",
    "    embedding_model = SentenceTransformer(\"intfloat/e5-large-v2\", device=embedding_device)\n",
    "\n",
    "    print(\"Training BERTopic model with customized settings...\")\n",
    "    # 1. Custom CountVectorizer for stop words and min_df\n",
    "    custom_stop_words = [\n",
    "        \"a\", \"an\", \"the\", \"and\", \"or\", \"but\",\n",
    "        \"if\", \"while\", \"is\", \"am\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\",\n",
    "        \"do\", \"does\", \"did\", \"doing\",\n",
    "        \"have\", \"has\", \"had\", \"having\",\n",
    "        \"i\", \"me\", \"my\", \"myself\",\n",
    "        \"we\", \"our\", \"ours\", \"ourselves\",\n",
    "        \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\",\n",
    "        \"he\", \"him\", \"his\", \"himself\",\n",
    "        \"she\", \"her\", \"hers\", \"herself\",\n",
    "        \"it\", \"its\", \"itself\",\n",
    "        \"they\", \"them\", \"their\", \"theirs\", \"themselves\",\n",
    "        \"this\", \"that\", \"these\", \"those\",\n",
    "        \"to\", \"in\", \"of\",\"for\",\"on\",\"at\",\"with\",\n",
    "        \"as\", \"such\", \"too\",\n",
    "        \"can\", \"will\", \"would\", \"should\", \"could\",\n",
    "        \"just\", \"only\", \"also\", \"so\", \"than\", \"then\", \"there\", \"here\",\n",
    "        \"what\", \"which\", \"who\", \"whom\", \"whose\",\n",
    "        \"when\", \"where\", \"why\", \"how\"\n",
    "    ]\n",
    "\n",
    "    vectorizer_model = CountVectorizer(stop_words=custom_stop_words, min_df=10, ngram_range=(1, 2))\n",
    "\n",
    "    # 2. Custom HDBSCAN for clustering\n",
    "    # Retaining the specific parameters for consistency and better topic formation\n",
    "    hdbscan_model = HDBSCAN(min_cluster_size=50, prediction_data=True)\n",
    "\n",
    "\n",
    "    # 3. Initialize BERTopic with all customized models and settings\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=embedding_model,\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        hdbscan_model=hdbscan_model,\n",
    "        nr_topics=\"auto\",\n",
    "        language=\"english\",\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    # Fit the model to your selected training data. This step trains the model.\n",
    "    topics_train, probabilities_train = topic_model.fit_transform(sampled_texts)\n",
    "    print(f\"BERTopic model training complete. Found {len(topic_model.get_topics())} topics.\")\n",
    "\n",
    "    # Save the trained model and topic information\n",
    "    print(f\"Saving BERTopic model to {MODEL_DIR}...\")\n",
    "    topic_model.save(MODEL_DIR)\n",
    "    print(f\"Saving topic summary to {TOPIC_JSON_PATH}...\")\n",
    "    save_topic_info(topic_model, TOPIC_JSON_PATH)\n",
    "\n",
    "    # --- Process ALL Data (including training data) in Chunks ---\n",
    "    print(\"Processing all reviews (including training data) in chunks and writing to DB...\")\n",
    "\n",
    "    with tqdm(total=len(df_full), desc=\"Processing All Reviews\", unit=\"reviews\") as pbar_full_analysis:\n",
    "        for start in range(0, len(df_full), CHUNK_SIZE):\n",
    "            end = start + CHUNK_SIZE\n",
    "            batch = df_full.iloc[start:end].copy()\n",
    "\n",
    "            texts_batch = batch[\"text\"].tolist()\n",
    "            topics_batch, probs_batch = topic_model.transform(texts_batch)\n",
    "\n",
    "            batch[\"topic\"] = topics_batch\n",
    "            batch[\"probability\"] = probs_batch\n",
    "\n",
    "            write_batch_to_db(engine, batch[[\"business_id\", \"review_id\", \"text\", \"topic\", \"probability\"]],\n",
    "                              OUTPUT_DB_TABLE, pg_schema)\n",
    "\n",
    "            pbar_full_analysis.update(len(batch))\n",
    "            print(f\"Processed and upserted {len(batch)} rows.\")\n",
    "\n",
    "    print(\"Finished topic modeling and database update for all reviews.\")\n",
    "\n",
    "    # Close the database connection\n",
    "    engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfe1045",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
