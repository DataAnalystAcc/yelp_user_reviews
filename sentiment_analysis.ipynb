{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88fdd260",
   "metadata": {},
   "source": [
    "Sentiment Analysis \n",
    "- RoBERTa\n",
    "- Modell cardiffnlp/twitter-roberta-base-sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2f1f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "from dotenv import dotenv_values\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm # Import tqdm for progress bars\n",
    "\n",
    "# Load environment variables from .env file\n",
    "config = dotenv_values()\n",
    "\n",
    "# --- Configuration Validation ---\n",
    "# Ensure all required environment variables are present before proceeding\n",
    "required_vars = ['POSTGRES_USER', 'POSTGRES_HOST', 'POSTGRES_PORT', 'POSTGRES_DB', 'POSTGRES_PASS']\n",
    "for var in required_vars:\n",
    "    if var not in config:\n",
    "        raise ValueError(f\"Missing required environment variable: {var}. Please check your .env file.\")\n",
    "\n",
    "# Assign environment variables to script variables\n",
    "pg_user = config['POSTGRES_USER']\n",
    "pg_host = config['POSTGRES_HOST']\n",
    "pg_port = config['POSTGRES_PORT']\n",
    "pg_db = config['POSTGRES_DB']\n",
    "# Use .get() for optional schema, defaulting to \"public\"\n",
    "pg_schema = config.get(\"POSTGRES_SCHEMA\", \"public\")\n",
    "pg_pass = config['POSTGRES_PASS']\n",
    "\n",
    "# --- Constants ---\n",
    "CHUNK_SIZE = 10_000 # Number of rows to fetch and process in each iteration\n",
    "BATCH_SIZE = 16    # Number of texts processed by the sentiment model in one batch\n",
    "TABLE_NAME = \"bertopic_analysis_results_update_2\" # Target database table\n",
    "sentiment_labels = ['negative', 'neutral', 'positive'] # Labels for sentiment output\n",
    "\n",
    "# --- Model Loading ---\n",
    "# Determine the device (GPU/CPU) for model computation\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using compute device: {device}\") # Inform the user which device is being used\n",
    "\n",
    "# Load the pre-trained sentiment tokenizer and model\n",
    "sentiment_tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "sentiment_model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "sentiment_model.to(device) # Move model to the selected device\n",
    "sentiment_model.eval() # Set the model to evaluation mode (disables dropout, etc.)\n",
    "\n",
    "# --- Database Connection Function ---\n",
    "def get_database_connection():\n",
    "    \"\"\"\n",
    "    Establishes and returns a SQLAlchemy engine for PostgreSQL database connectivity.\n",
    "    Configured with pool_pre_ping to ensure connections are healthy.\n",
    "    \"\"\"\n",
    "    url = f\"postgresql+psycopg2://{pg_user}:{pg_pass}@{pg_host}:{pg_port}/{pg_db}\"\n",
    "    return create_engine(url, pool_pre_ping=True)\n",
    "\n",
    "# --- Database Schema Management Function ---\n",
    "def ensure_sentiment_columns(engine):\n",
    "    \"\"\"\n",
    "    Ensures that 'sentiment_label' (VARCHAR) and 'sentiment_score' (FLOAT) columns\n",
    "    exist in the specified database table. If they already exist, they are dropped\n",
    "    and then re-added to ensure a clean slate for sentiment analysis.\n",
    "    \"\"\"\n",
    "    with engine.connect() as conn:\n",
    "        # Check for existing columns by querying information_schema\n",
    "        result = conn.execute(text(f\"\"\"\n",
    "            SELECT column_name\n",
    "            FROM information_schema.columns\n",
    "            WHERE table_schema = :schema_name AND table_name = :table_name\n",
    "        \"\"\"), {\"schema_name\": pg_schema, \"table_name\": TABLE_NAME})\n",
    "        existing_cols = {row[0] for row in result}\n",
    "\n",
    "        drop_statements = []\n",
    "        if 'sentiment_label' in existing_cols:\n",
    "            drop_statements.append('DROP COLUMN sentiment_label')\n",
    "        if 'sentiment_score' in existing_cols:\n",
    "            drop_statements.append('DROP COLUMN sentiment_score')\n",
    "\n",
    "        if drop_statements:\n",
    "            # If columns exist, construct and execute the ALTER TABLE DROP COLUMN statement\n",
    "            drop_sql = f'ALTER TABLE \"{pg_schema}\".\"{TABLE_NAME}\" ' + ', '.join(drop_statements) + \";\"\n",
    "            conn.execute(text(drop_sql))\n",
    "            print(f\"Deleted existing sentiment columns in '{TABLE_NAME}'.\")\n",
    "\n",
    "        # Construct and execute the ALTER TABLE ADD COLUMN statement for new columns\n",
    "        add_sql = f\"\"\"\n",
    "            ALTER TABLE \"{pg_schema}\".\"{TABLE_NAME}\"\n",
    "            ADD COLUMN sentiment_label VARCHAR(10),\n",
    "            ADD COLUMN sentiment_score FLOAT;\n",
    "        \"\"\"\n",
    "        conn.execute(text(add_sql))\n",
    "        conn.commit() # Commit the DDL changes to the database\n",
    "        print(f\"Created new sentiment columns in '{TABLE_NAME}'.\")\n",
    "\n",
    "# --- Sentiment Analysis Function ---\n",
    "def analyze_sentiment(texts):\n",
    "    \"\"\"\n",
    "    Performs sentiment analysis on a list of input texts using the pre-loaded\n",
    "    Hugging Face model. Texts are processed in batches for efficiency.\n",
    "    Returns lists of predicted sentiment labels and their corresponding scores.\n",
    "    \"\"\"\n",
    "    all_labels = []\n",
    "    all_scores = []\n",
    "\n",
    "    # Iterate through texts in batches, using tqdm for a nested progress bar\n",
    "    # 'desc' provides a description, 'leave=False' makes the bar disappear on completion\n",
    "    for i in tqdm(range(0, len(texts), BATCH_SIZE), desc=\"Analyzing Sentiment Batches\", leave=False):\n",
    "        batch_texts = texts[i:i+BATCH_SIZE]\n",
    "        # Tokenize the batch, ensuring padding, truncation, and returning PyTorch tensors\n",
    "        inputs = sentiment_tokenizer(batch_texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "        # Move input tensors to the specified compute device (CPU/CUDA)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad(): # Context manager to disable gradient calculation for inference\n",
    "            outputs = sentiment_model(**inputs)\n",
    "            # Apply softmax to the model's logits to get probability distributions\n",
    "            probs = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "            # Get the maximum probability (score) and its index (label) for each text\n",
    "            scores, labels = torch.max(probs, dim=1)\n",
    "\n",
    "        # Extend the global lists with the results, converting tensors to numpy arrays\n",
    "        all_labels.extend([sentiment_labels[label] for label in labels.cpu().numpy()])\n",
    "        all_scores.extend(scores.cpu().numpy())\n",
    "\n",
    "    return all_labels, all_scores\n",
    "\n",
    "# --- Main Logic ---\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to execute the sentiment analysis pipeline:\n",
    "    1. Connects to the database.\n",
    "    2. Ensures sentiment columns are ready.\n",
    "    3. Fetches review data in chunks.\n",
    "    4. Performs sentiment analysis on each chunk.\n",
    "    5. Updates the database with the sentiment results.\n",
    "    6. Provides progress updates using tqdm.\n",
    "    \"\"\"\n",
    "    # Initialize the database engine once at the start of the main function\n",
    "    engine = get_database_connection()\n",
    "\n",
    "    try:\n",
    "        # Ensure the necessary sentiment columns exist in the database table\n",
    "        ensure_sentiment_columns(engine)\n",
    "\n",
    "        # Get the total count of ALL rows in the table\n",
    "        # This count is used to initialize the main progress bar for processing all rows\n",
    "        with engine.connect() as conn:\n",
    "            total_rows_to_process = conn.execute(text(f\"\"\"\n",
    "                SELECT COUNT(*) FROM \"{pg_schema}\".\"{TABLE_NAME}\"\n",
    "            \"\"\")).scalar_one() # .scalar_one() retrieves a single scalar result (e.g., the count)\n",
    "\n",
    "        print(f\"Starting sentiment analysis for all {total_rows_to_process} rows in '{TABLE_NAME}'.\")\n",
    "\n",
    "        offset = 0 # Initialize offset for fetching data in chunks\n",
    "        total_processed_in_session = 0 # Counter for rows processed in the current run\n",
    "\n",
    "        # Initialize the main progress bar for the overall sentiment analysis process\n",
    "        # 'position=0' helps control the order of nested tqdm bars in the console\n",
    "        with tqdm(total=total_rows_to_process, desc=\"Overall Sentiment Analysis\", position=0, unit=\"rows\") as pbar_main:\n",
    "            while True:\n",
    "                df = pd.DataFrame() # Initialize an empty DataFrame for each iteration's chunk\n",
    "\n",
    "                # Fetch a chunk of data from the database (now fetching ALL rows, not just NULL ones)\n",
    "                with engine.connect() as conn:\n",
    "                    query = text(f\"\"\"\n",
    "                        SELECT review_id, text\n",
    "                        FROM \"{pg_schema}\".\"{TABLE_NAME}\"\n",
    "                        ORDER BY review_id\n",
    "                        LIMIT :limit OFFSET :offset\n",
    "                    \"\"\")\n",
    "                    # Use pandas.read_sql to execute the query and load data into a DataFrame\n",
    "                    df = pd.read_sql(query, conn, params={\"limit\": CHUNK_SIZE, \"offset\": offset})\n",
    "\n",
    "                if df.empty:\n",
    "                    # If the DataFrame is empty, it means no more reviews were found\n",
    "                    print(\"All reviews processed for sentiment in the database.\")\n",
    "                    break # Exit the loop\n",
    "\n",
    "                current_chunk_size = len(df)\n",
    "                print(f\"Processing chunk with offset {offset}, size {current_chunk_size} reviews...\")\n",
    "\n",
    "                # Perform sentiment analysis on the 'text' column of the current chunk\n",
    "                sentiments, scores = analyze_sentiment(df[\"text\"].tolist())\n",
    "                # Add the sentiment results back to the DataFrame\n",
    "                df[\"sentiment_label\"] = sentiments\n",
    "                df[\"sentiment_score\"] = scores\n",
    "\n",
    "                # Update the database with the sentiment results for the current chunk\n",
    "                # Use engine.begin() for an atomic transaction for all updates within this chunk\n",
    "                with engine.begin() as conn:\n",
    "                    # Use tqdm for a nested progress bar to show database update progress for the current chunk\n",
    "                    # 'leave=False' makes this bar disappear upon completion of each chunk's update\n",
    "                    # 'position=1' ensures it appears below the main progress bar\n",
    "                    for _, row in tqdm(df.iterrows(), total=current_chunk_size, desc=\"Updating DB for chunk\", leave=False, position=1):\n",
    "                        update_stmt = text(f\"\"\"\n",
    "                            UPDATE \"{pg_schema}\".\"{TABLE_NAME}\"\n",
    "                            SET sentiment_label = :label,\n",
    "                                sentiment_score = :score\n",
    "                            WHERE review_id = :rid\n",
    "                        \"\"\")\n",
    "                        # Execute the update statement with parameters from the current row\n",
    "                        conn.execute(update_stmt, {\n",
    "                            \"label\": row[\"sentiment_label\"],\n",
    "                            \"score\": float(row[\"sentiment_score\"]), # Ensure score is explicitly float\n",
    "                            \"rid\": row[\"review_id\"]\n",
    "                        })\n",
    "\n",
    "                # After successfully processing and updating a chunk, update the main progress bar\n",
    "                pbar_main.update(current_chunk_size)\n",
    "                total_processed_in_session += current_chunk_size\n",
    "                offset += CHUNK_SIZE # Increment offset to fetch the next chunk in the next iteration\n",
    "\n",
    "                print(f\"Chunk processed. Total rows analyzed and updated in this session: {total_processed_in_session}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # Catch any exceptions that occur during the process and print an error message\n",
    "        print(f\"An error occurred during sentiment analysis: {e}\")\n",
    "    finally:\n",
    "        # Ensure the database engine is properly disposed of, regardless of success or failure\n",
    "        if engine:\n",
    "            engine.dispose()\n",
    "            print(\"Database engine disposed.\")\n",
    "\n",
    "# Entry point of the script\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
